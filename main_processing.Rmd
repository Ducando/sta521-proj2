---
title: "521 Project 2"
output: html_document
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(viridis)
library(GGally)
library(tidymodels)
# library(patchwork)
library(ggpubr)
source("CVmaster.R")
options(scipen = 999)


# graph theme object 
theme_settings <- theme_light() + theme(axis.text = element_text(size=8), axis.title = element_text(size = 9), legend.title = element_text(size = 9), plot.title = element_text(size = 10, hjust = 0.5)) 
```

## Section 1 

&nbsp; 

```{r}
# read in files with column names 
names <- c("y_coord", "x_coord", "expert", "ndai", "sd", "corr", "angle_df", 
           "angle_cf", "angle_bf", "angle_af", "angle_an")

imagem1 <- read.table('./data/imagem1.txt', col.names = names)
imagem2 <- read.table('./data/imagem2.txt', col.names = names)
imagem3 <- read.table('./data/imagem3.txt', col.names = names)


# make expert column factor and text version
imagem1 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem1

imagem2 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem2

imagem3 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem3

imagem1 %>%
  bind_rows(imagem2) %>%
  bind_rows(imagem3) -> all

```

&nbsp; 

#### Part B

&nbsp; 

```{r}
# # look at percentage for total 
# all %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / 345556),2))
# 
# # look at percentage for each one 
# len1 <- nrow(imagem1)
# imagem1 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len1),2))
# 
# len2 <- nrow(imagem2)
# imagem2 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len2),2))
# 
# len3 <- nrow(imagem3)
# imagem3 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len3),2))
```


```{r}

# We might want to change the colors here becuase it's hard to distinguish between the blue and the black on the legend

# Image 1
# ggplot(imagem1, aes(x = x_coord, y = y_coord, color = expert_text)) + 
#   geom_point() + 
#   theme_settings + 
#   labs(x = "X Coordinate", y = "Y Coordinate", color = "Expert Labels") +
#   scale_colour_manual(values = c("darkblue", "black", "lightgrey"))
# 
# # Image 2
# ggplot(imagem2, aes(x = x_coord, y = y_coord, color = expert_text)) + 
#   geom_point() + 
#   theme_settings + 
#   labs(x = "X Coordinate", y = "Y Coordinate", color = "Expert Labels") +
#   scale_colour_manual(values = c("darkblue", "black", "lightgrey"))
# 
# # Image 3
# ggplot(imagem3, aes(x = x_coord, y = y_coord, color = expert_text)) + 
#   geom_point() + 
#   theme_settings + 
#   labs(x = "X Coordinate", y = "Y Coordinate", color = "Expert Labels") +
#   scale_colour_manual(values = c("darkblue", "black", "lightgrey"))
```

&nbsp; 

#### Part C

&nbsp; 

```{r}
# pairwise relationships between features 
# cor(all$ndai, all$corr)

# all %>%
#   select(ndai:angle_an) %>%
#   ggpairs()
```


```{r}
# expert labels vs features 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = ndai, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = corr, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = sd, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_df, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_cf, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_bf, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_af, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_an, fill= expert)) +
#   geom_histogram()
```


## Section 2

#### Part A

```{r}
set.seed(513)
images <- list(imagem1,imagem2,imagem3)
indices <- sample(1:3)
train_image <- images[[indices[1]]]
val_image <- images[[indices[2]]]
test_image <- images[[indices[3]]]
```

```{r}
find_blocks <- function(x, y, num_blocks) {
  n <- sqrt(num_blocks)
  x_min <- min(x)
  x_max <- max(x)
  x_step <- (x_max-x_min)/n
  
  y_min <- min(y)
  y_max <- max(y)
  y_step <- (y_max-y_min)/n
  
  block <- tibble()
  names(block) <- c("top", "bottom", "left", "right")
  
  for (i in seq_len(n)) {
    # make top and bottom
    top <- floor(y_min + (i-1) * y_step)
    bot <- floor(y_max - (n-i) * y_step )
    if (i != n)
      bot <- bot - 1
    
    for (j in seq_len(n)){
      # make left and right 
      left <- floor(x_min + (j-1) * x_step)
      right <- floor(x_max - (n-j) * x_step )
      if (j != n)
        right <- right - 1
      
      block %>%
        bind_rows(data.frame(top, bot, left, right)) -> block 
    }
  }
  
  return(block)
}

# this is going to return overlapping intervals, so will need x >= top and x < bot, etc. 
```



```{r}
split_blocks <- function(block, num_blocks, train_num_blocks, val_num_blocks) {
  set.seed(513)
  train_index <- sample(seq_len(num_blocks),train_num_blocks)
  set.seed(513)
  val_index <- sample(setdiff(seq_len(num_blocks),train_index),val_num_blocks)
  test_index <- setdiff(seq_len(num_blocks), c(train_index,val_index))
  
  train_blocks <- block[train_index,]
  val_blocks <- block[val_index,]
  test_blocks <- block[test_index,]
  
  return(list(train = train_blocks, val = val_blocks, test = test_blocks))
}
```

```{r}
split_data <- function(data, block) {
  final_data <- tibble()
  for (i in seq_len(nrow(block))) {
    coords <- block[i,]
    
    data %>%
      filter(x_coord >= coords$left, x_coord <= coords$right,
            y_coord >= coords$top, y_coord <= coords$bot) -> filtered_data
        
    final_data <- rbind(final_data, filtered_data)
  }
  
  return(final_data)
}
```


```{r}
process_data_main <- function(df, num_blocks, train_num_blocks, val_num_blocks) {
  blocks <- find_blocks(df$x_coord, df$y_coord, num_blocks)
  block_indices <- split_blocks(blocks, num_blocks, train_num_blocks, val_num_blocks)
  
  val <- split_data(df, block_indices$val)
  test <- split_data(df, block_indices$test)
  train <- split_data(df, block_indices$train)

  return(list(val = val, test = test, train = train))
}
```


```{r}
image1_dfs <- process_data_main(imagem1, 16, 10, 3)
image2_dfs <- process_data_main(imagem2, 16, 10, 3)
image3_dfs <- process_data_main(imagem3, 16, 10, 3)

train <- image1_dfs$train %>%
  bind_rows(image2_dfs$train) %>%
  bind_rows(image3_dfs$train)

test <- image1_dfs$test%>%
  bind_rows(image2_dfs$test) %>%
  bind_rows(image3_dfs$test)


val <- image1_dfs$val%>%
  bind_rows(image2_dfs$val) %>%
  bind_rows(image3_dfs$val)

# 345556
# nrow(train) + nrow(test) + nrow(val)
```

#### Part B
```{r}
val_image %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
test_image %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))

val %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
test %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
```
  
This classifier will have high average accuracy when the image is mostly cloud free.

#### Part C
```{r}
all %>%
  select(ndai:angle_an) %>%
  cor(.,as.integer(all$expert))
```

```{r}
imagem1 %>%
  filter(expert != 0) %>%
  ggplot(aes(x = sd, fill= expert)) +
  geom_histogram()
```
    
NDAI and CORR are selected as best features becuase they have the highest correlation values with the expert labels. SD is also selected because the range of values between the cloudy and cloud free data is more separable than the range of values for the angles.

#### Part D
See the CVmaster.R script.

## Section 3

#### Part A

```{r}
# get x and y train data for each data split
X_train_image <- train_image %>%
  bind_rows(val_image) %>%
  filter(expert != 0) %>%
  select(-y_coord, -x_coord, -expert_text, -expert)

y_train_image  <- train_image %>%
  bind_rows(val_image) %>%
  filter(expert!=0) %>%
  .$expert %>%
  droplevels()

X_train_blocks <- train %>%
  bind_rows(val) %>%
  filter(expert != 0) %>%
  select(-y_coord, -x_coord, -expert_text, -expert)

y_train_blocks  <- train %>%
  bind_rows(val) %>%
  filter(expert!=0) %>%
  .$expert %>%
  droplevels()
```


First modelling by splitting the data by image and by block 
```{r}
## RUN IMAGE MODELS 

# LOGISTIC 
image_log <- CVmaster(model = "logistic", 
                       X = X_train_image, y =y_train_image, 
                       k = 10, loss = c("accuracy"), 
                       estimates = c("roc", "conf_mat", "precision"))

# RF
image_rf <- CVmaster(model = "rf",
                      X = X_train_image, y = y_train_image,
                      k = 10, loss = c("accuracy"), 
                      estimates = c("roc", "conf_mat", "precision"))

# BOOSTED TREES 
image_boost <- CVmaster(model = "boosted_trees", 
                         X = X_train_image, y = y_train_image,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 


# LDA 
image_lda <- CVmaster(model = "lda", 
                         X = X_train_image, y = y_train_image,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
# QDA 
image_qda <- CVmaster(model = "qda", 
                         X = X_train_image, y = y_train_image,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision"))


## RUN BLOCK MODELS 
# LOGISTIC 
block_log <- CVmaster(model = "logistic", 
                       X = X_train_blocks, y =y_train_blocks, 
                       k = 10, loss = c("accuracy"), 
                       estimates = c("roc", "conf_mat", "precision"))
# RF
block_rf <- CVmaster(model = "rf", 
                      X = X_train_blocks, y = y_train_blocks,
                      k = 10, loss = c("accuracy"), 
                      estimates = c("roc", "conf_mat", "precision"))

# BOOSTED TREES 
block_boost <- CVmaster(model = "boosted_trees", 
                         X = X_train_blocks, y = y_train_blocks,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
# LDA 
block_lda <- CVmaster(model = "lda", 
                         X = X_train_blocks, y = y_train_blocks,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
# QDA 
block_qda <- CVmaster(model = "qda", 
                         X = X_train_blocks, y = y_train_blocks,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
```


```{r}
## GET ACCURACY ESTIMATES BY MODEL 
image_log$accuracy %>%
  bind_rows(image_rf$accuracy) %>%
  bind_rows(image_boost$accuracy) %>%
  bind_rows(image_lda$accuracy) %>%
  bind_rows(image_qda$accuracy) %>%
  mutate(type = "Image") %>%
  bind_rows(block_log$accuracy) %>%
  bind_rows(block_rf$accuracy) %>%
  bind_rows(block_boost$accuracy) %>%
  bind_rows(block_lda$accuracy) %>%
  bind_rows(block_qda$accuracy) %>%
  mutate(type = ifelse(is.na(type), "Block", type)) -> model_sum

model_sum %>%
  dplyr::select(-mtry, -min_n, -penalty, -metric) %>%
  group_by(type,model) %>%
  rename(accuracy = mean) %>%
  mutate(mean_accuracy = mean(accuracy)) %>%
  pivot_wider(names_from = id, values_from = accuracy) %>%
  arrange(desc(mean_accuracy)) %>%
  rename(`Mean Accuracy` = mean_accuracy)%>%
  rename_with(str_to_title) -> model_accuracy_table

model_sum %>%
  ungroup() %>%
  dplyr::select(-metric, -mean, -id) %>%
  distinct() %>%
  rename(`Min Try` = mtry, 
         `Min Nodes` = min_n) %>%
  rename_with(str_to_title) %>%
  # rename(`Min Nodes` = min_n) %>%
  pivot_longer(cols = c(Penalty, `Min Try`, `Min Nodes`), names_to = "Parameter", values_to = "Value") %>%
  mutate(Value = round(Value, 6)) %>%
  filter(!is.na(Value))

  
```


#### Part B 
```{r fig.width=12, fig.height=6}
## GET ROC CURVES BY MODEL 
image_roc_plot <- image_log$roc %>%
  bind_rows(image_rf$roc) %>%
  bind_rows(image_boost$roc) %>%
  bind_rows(image_lda$roc) %>%
  bind_rows(image_qda$roc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() +
  theme_settings + 
  labs(main = "Image ROC", x = "", y = "Sensitivity", col = "Model")


block_roc_plot<-  block_log$roc %>%
  bind_rows(block_rf$roc) %>%
  bind_rows(block_boost$roc) %>%
    bind_rows(block_lda$roc) %>%
  bind_rows(block_qda$roc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() + 
  theme_settings +
  labs(main = "Block ROC", x = "1 - Specificity", y = "Sensitivity", col = "Model")

# change colors, linetype, size
image_roc_plot + block_roc_plot + plot_layout(guides = "collect")

```

#### Part C 
```{r}
## PRECISION ESTIMATES 
image_log$precision

## CONF MATRIX

```



## Section 4
```{r}
rf_train_image <- train_image %>%
  filter(expert != 0) %>%
  select(-y_coord, -x_coord, -expert_text)

rf_val_image <- val_image %>%
  filter(expert != 0) %>%
  select(-y_coord, -x_coord, -expert_text)
```


```{r}
## image data
#
#rf <- rand_forest(mtry = tune(), min_n = tune(), trees = 50) %>%
#  set_engine("ranger") %>%
#  set_mode("classification")
#    
#rf_recipe <- recipe(expert ~., data = rf_train_image)
#
#workflow <- workflow() %>%
#  add_recipe(rf_recipe) %>%
#  add_model(rf) 
#    
#res <- workflow %>%
#  tune_grid(rf_val_image,X_valgrid = 25, 
#            control = control_grid(save_pred = TRUE))
#
#workflow %>%
#  tune_grid(rf_val_image, grid = 25, 
#            control = control_grid(save_pred = TRUE))
```
```{r}
library(ranger)
```


```{r}
#mtrys <- 1:8
#min_ns <- 1:10

mtrys <- 1:8
min_ns <- 1:10

image_acc <- matrix(NA,nrow=length(mtrys),ncol=length(min_ns))
for (i in seq_along(mtrys)){
  for (j in seq_along(min_ns)){
    mod <- ranger(expert ~., data = rf_train_image,mtry = mtrys[i], min_n = min_ns[j], trees = 50)
    preds <- predict(mod,data=rf_val_image)
    image_acc[i,j] <- mean(rf_val_image$expert==preds$predictions)
  }
}
```


```{r}
rf_train_block <- train %>%
  filter(expert != 0) %>%
  select(-y_coord, -x_coord, -expert_text)

rf_val_block <- val %>%
  filter(expert != 0) %>%
  select(-y_coord, -x_coord, -expert_text)

rf_test_block <- test %>%
  filter(expert != 0) %>%
  select(-y_coord, -x_coord, -expert_text)
```

```{r}
mtrys <- 1:8
min_ns <- 1:10

block_acc <- matrix(NA,nrow=length(mtrys),ncol=length(min_ns))
for (i in seq_along(mtrys)){
  for (j in seq_along(min_ns)){
    mod <- ranger(expert ~., data = rf_train_block,mtry = mtrys[i], min_n = min_ns[j], trees = 50)
    preds <- predict(mod,data=rf_val_block)
    block_acc[i,j] <- mean(rf_val_block$expert==preds$predictions)
  }
}
```

```{r}
plot(1:8,apply(block_acc,1,mean),type="l",col="#56B1F7",xlim=c(1,10))
lines(1:10,apply(block_acc,2,mean),col="#132B43")
```
Based on this, picking 4 for both mtry and min_n (0.9138478)

```{r}
rf_mod <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 100)
preds <- predict(mod,data=rf_val_block)
mean(rf_val_block$expert==preds$predictions)
```

```{r}
rf_mod <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 500)
preds <- predict(mod,data=rf_val_block)
mean(rf_val_block$expert==preds$predictions)
```

Increasing number of trees does not significantly change predictions

```{r}
set.seed(513)
final_mod <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 50,importance = 'impurity')
val_preds <- predict(final_mod,data=rf_val_block)
mean(rf_val_block$expert==val_preds$predictions)

test_preds <- predict(final_mod,data=rf_test_block)
mean(rf_test_block$expert==test_preds$predictions)
```

```{r}
final_mod_prob <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 50,probability = TRUE)
test_prob_preds <- predict(final_mod_prob,data=rf_test_block)
```

```{r}
var_imp <- tibble(variable=names(final_mod$variable.importance),importance = final_mod$variable.importance)

 ggplot(var_imp, aes(x=reorder(variable,importance), y=importance,fill=importance))+ 
      geom_bar(stat="identity", position="dodge")+ coord_flip()+
      ylab("Variable Importance")+
      xlab("")+
      guides(fill=F)
```
```{r}
image_1_data <- imagem1 %>%
  select(-y_coord, -x_coord, -expert_text)
image_1_class <- predict(final_mod,data=image_1_data)$predictions
image_1_pred_class <- tibble(x_coord = imagem1$x_coord, y_coord = imagem1$y_coord, class = image_1_class)
ggplot(image_1_pred_class, aes(x_coord,y_coord, fill = class)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))

image_1_preds <- predict(final_mod_prob,data=image_1_data)$predictions
image_1_probs <- tibble(x_coord = imagem1$x_coord, y_coord = imagem1$y_coord, prob = image_1_preds[,2])
ggplot(image_1_probs, aes(x_coord,y_coord, fill = prob)) + 
     geom_raster()
```

```{r}
image_2_data <- imagem2 %>%
  select(-y_coord, -x_coord, -expert_text)
image_2_class <- predict(final_mod,data=image_2_data)$predictions
image_2_pred_class <- tibble(x_coord = imagem2$x_coord, y_coord = imagem2$y_coord, class = image_2_class)
ggplot(image_2_pred_class, aes(x_coord,y_coord, fill = class)) + 
     geom_raster()+scale_fill_manual(values = c("#132B43", "#56B1F7"))

image_2_preds <- predict(final_mod_prob,data=image_2_data)$predictions
image_2_probs <- tibble(x_coord = imagem2$x_coord, y_coord = imagem2$y_coord, prob = image_2_preds[,2])
ggplot(image_2_probs, aes(x_coord,y_coord, fill = prob)) + 
     geom_raster()
```

```{r}
image_3_data <- imagem3 %>%
  select(-y_coord, -x_coord, -expert_text)
image_3_class <- predict(final_mod,data=image_3_data)$predictions
image_3_pred_class <- tibble(x_coord = imagem3$x_coord, y_coord = imagem3$y_coord, class = image_3_class)
ggplot(image_3_pred_class, aes(x_coord,y_coord, fill = class)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))

image_3_preds <- predict(final_mod_prob,data=image_3_data)$predictions
image_3_probs <- tibble(x_coord = imagem3$x_coord, y_coord = imagem3$y_coord, prob = image_3_preds[,2])
ggplot(image_3_probs, aes(x_coord,y_coord, fill = prob)) + 
     geom_raster()
```

```{r}
image_1_pred_class <- image_1_pred_class %>%
  mutate(misclassified = ifelse(imagem1$expert != 0 & class != imagem1$expert,"Yes","No"))

ggplot(image_1_pred_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r}
image_2_pred_class <- image_2_pred_class %>%
  mutate(misclassified = ifelse(imagem2$expert != 0 & class != imagem2$expert,"Yes","No"))

ggplot(image_2_pred_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r}
image_3_pred_class <- image_3_pred_class %>%
  mutate(misclassified = ifelse(imagem3$expert != 0 & class != imagem3$expert,"Yes","No"))

ggplot(image_3_pred_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r}
image_1_pred_class <- image_1_pred_class %>%
  mutate(misclassification = ifelse(imagem1$expert != 0 & class != imagem1$expert,1,.2))
ggplot(image_1_pred_class, aes(x_coord,y_coord, fill = class,alpha=misclassification)) + 
  geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))+ scale_alpha_identity()

image_2_pred_class <- image_2_pred_class %>%
  mutate(misclassification = ifelse(imagem2$expert != 0 & class != imagem2$expert,1,.2))
ggplot(image_2_pred_class, aes(x_coord,y_coord, fill = class,alpha=misclassification)) + 
  geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))+ scale_alpha_identity()

image_3_pred_class <- image_3_pred_class %>%
  mutate(misclassification = ifelse(imagem3$expert != 0 & class != imagem3$expert,1,.2))
ggplot(image_3_pred_class, aes(x_coord,y_coord, fill = class,alpha=misclassification)) + 
  geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))+ scale_alpha_identity()
```



