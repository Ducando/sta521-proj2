---
title: "521 Project 2"
output: html_document
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(viridis)
library(GGally)
library(tidymodels)
source("CVmaster.R")
source("CVmaster2.R")


# graph theme object 
theme_settings <- theme_light() + theme(axis.text = element_text(size=8), axis.title = element_text(size = 9), legend.title = element_text(size = 9), plot.title = element_text(size = 10, hjust = 0.5)) 
```

## Section 1 

&nbsp; 

```{r}
# read in files with column names 
names <- c("y_coord", "x_coord", "expert", "ndai", "sd", "corr", "angle_df", 
           "angle_cf", "angle_bf", "angle_af", "angle_an")

imagem1 <- read.table('./data/imagem1.txt', col.names = names)
imagem2 <- read.table('./data/imagem2.txt', col.names = names)
imagem3 <- read.table('./data/imagem3.txt', col.names = names)


# make expert column factor and text version
imagem1 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem1

imagem2 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem2

imagem3 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem3

imagem1 %>%
  bind_rows(imagem2) %>%
  bind_rows(imagem3) -> all

```

&nbsp; 

#### Part B

&nbsp; 

```{r}
# # look at percentage for total 
# all %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / 345556),2))
# 
# # look at percentage for each one 
# len1 <- nrow(imagem1)
# imagem1 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len1),2))
# 
# len2 <- nrow(imagem2)
# imagem2 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len2),2))
# 
# len3 <- nrow(imagem3)
# imagem3 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len3),2))
```


```{r}

# We might want to change the colors here becuase it's hard to distinguish between the blue and the black on the legend

# Image 1
# ggplot(imagem1, aes(x = x_coord, y = y_coord, color = expert_text)) + 
#   geom_point() + 
#   theme_settings + 
#   labs(x = "X Coordinate", y = "Y Coordinate", color = "Expert Labels") +
#   scale_colour_manual(values = c("darkblue", "black", "lightgrey"))
# 
# # Image 2
# ggplot(imagem2, aes(x = x_coord, y = y_coord, color = expert_text)) + 
#   geom_point() + 
#   theme_settings + 
#   labs(x = "X Coordinate", y = "Y Coordinate", color = "Expert Labels") +
#   scale_colour_manual(values = c("darkblue", "black", "lightgrey"))
# 
# # Image 3
# ggplot(imagem3, aes(x = x_coord, y = y_coord, color = expert_text)) + 
#   geom_point() + 
#   theme_settings + 
#   labs(x = "X Coordinate", y = "Y Coordinate", color = "Expert Labels") +
#   scale_colour_manual(values = c("darkblue", "black", "lightgrey"))
```

&nbsp; 

#### Part C

&nbsp; 

```{r}
# pairwise relationships between features 
# cor(all$ndai, all$corr)

# all %>%
#   select(ndai:angle_an) %>%
#   ggpairs()
```


```{r}
# expert labels vs features 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = ndai, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = corr, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = sd, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_df, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_cf, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_bf, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_af, fill= expert)) +
#   geom_histogram()
# 
# imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_an, fill= expert)) +
#   geom_histogram()
```


## Section 2

#### Part A

```{r}
set.seed(513)
images <- list(imagem1,imagem2,imagem3)
indices <- sample(1:3)
train_image <- images[[indices[1]]]
val_image <- images[[indices[2]]]
test_image <- images[[indices[3]]]
```

```{r}
find_blocks <- function(x, y, num_blocks) {
  n <- sqrt(num_blocks)
  x_min <- min(x)
  x_max <- max(x)
  x_step <- (x_max-x_min)/n
  
  y_min <- min(y)
  y_max <- max(y)
  y_step <- (y_max-y_min)/n
  
  block <- tibble()
  names(block) <- c("top", "bottom", "left", "right")
  
  for (i in seq_len(n)) {
    # make top and bottom
    top <- floor(y_min + (i-1) * y_step)
    bot <- floor(y_max - (n-i) * y_step )
    if (i != n)
      bot <- bot - 1
    
    for (j in seq_len(n)){
      # make left and right 
      left <- floor(x_min + (j-1) * x_step)
      right <- floor(x_max - (n-j) * x_step )
      if (j != n)
        right <- right - 1
      
      block %>%
        bind_rows(data.frame(top, bot, left, right)) -> block 
    }
  }
  
  return(block)
}

# this is going to return overlapping intervals, so will need x >= top and x < bot, etc. 
```



```{r}
split_blocks <- function(block, num_blocks, train_num_blocks, val_num_blocks) {
  set.seed(513)
  train_index <- sample(seq_len(num_blocks),train_num_blocks)
  set.seed(513)
  val_index <- sample(setdiff(seq_len(num_blocks),train_index),val_num_blocks)
  test_index <- setdiff(seq_len(num_blocks), c(train_index,val_index))
  
  train_blocks <- block[train_index,]
  val_blocks <- block[val_index,]
  test_blocks <- block[test_index,]
  
  return(list(train = train_blocks, val = val_blocks, test = test_blocks))
}
```

```{r}
split_data <- function(data, block) {
  final_data <- tibble()
  for (i in seq_len(nrow(block))) {
    coords <- block[i,]
    
    data %>%
      filter(x_coord >= coords$left, x_coord <= coords$right,
            y_coord >= coords$top, y_coord <= coords$bot) -> filtered_data
        
    final_data <- rbind(final_data, filtered_data)
  }
  
  return(final_data)
}
```


```{r}
process_data_main <- function(df, num_blocks, train_num_blocks, val_num_blocks) {
  blocks <- find_blocks(df$x_coord, df$y_coord, num_blocks)
  block_indices <- split_blocks(blocks, num_blocks, train_num_blocks, val_num_blocks)
  
  val <- split_data(df, block_indices$val)
  test <- split_data(df, block_indices$test)
  train <- split_data(df, block_indices$train)

  return(list(val = val, test = test, train = train))
}
```


```{r}
image1_dfs <- process_data_main(imagem1, 16, 10, 3)
image2_dfs <- process_data_main(imagem2, 16, 10, 3)
image3_dfs <- process_data_main(imagem3, 16, 10, 3)

train <- image1_dfs$train %>%
  bind_rows(image2_dfs$train) %>%
  bind_rows(image3_dfs$train)

test <- image1_dfs$test%>%
  bind_rows(image2_dfs$test) %>%
  bind_rows(image3_dfs$test)


val <- image1_dfs$val%>%
  bind_rows(image2_dfs$val) %>%
  bind_rows(image3_dfs$val)

# 345556
# nrow(train) + nrow(test) + nrow(val)
```

#### Part B
```{r}
val %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
test %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
```
  
This classifier will have high average accuracy when the image is mostly cloud free.

#### Part C
```{r}
all %>%
  select(ndai:angle_an) %>%
  cor(.,as.integer(all$expert))
```

```{r}
imagem1 %>%
  filter(expert != 0) %>%
  ggplot(aes(x = sd, fill= expert)) +
  geom_histogram()
```
    
NDAI and CORR are selected as best features becuase they have the highest correlation values with the expert labels. SD is also selected because the range of values between the cloudy and cloud free data is more separable than the range of values for the angles.

#### Part D
See the CVmaster.R script.

## Section 3

#### Part A

```{r}
X_train_image <- train_image %>%
  bind_rows(val_image) %>%
  filter(expert != 0) %>%
  select(ndai,sd,corr)
y_train_image  <- train_image %>%
  bind_rows(val_image) %>%
  filter(expert!=0) %>%
  .$expert %>%
  droplevels()
```


```{r}
X_train_blocks <- train %>%
  bind_rows(val) %>%
  filter(expert != 0) %>%
  select(ndai,sd,corr)
y_train_blocks  <- train %>%
  bind_rows(val) %>%
  filter(expert!=0) %>%
  .$expert %>%
  droplevels()
```


```{r}
# returns one row for each fold across number of folds

# some function to loop through folds, returns list of mses
# loop_folds <- function(model, x, y, max_folds, loss, ntree = NA) {
#   
#   mses <- tibble()
#   
#   for (folds in seq(from =2, to = max_folds, by = 1)){
#     results <- CVmaster(model, x, y, folds, loss, ntree)
#     mses <- bind_rows(mses, c(num_folds = folds, fold = list(seq_len(folds)), val_error = list(results)))
#     message(sprintf("Finished for %.0f folds\n", folds))
#   }
#   
#   return(mses)
# }
```

First modelling by splitting the data by image and by block 
```{r}
## RUN IMAGE MODELS 

# LOGISTIC 
image_log <- CVmaster2(model = "logistic", X = X_train_image, y =y_train_image, 
                       k = 10, loss = c("accuracy", "roc"))
# RF
image_rf <- CVmaster2(model = "rf", X = X_train_image, y = y_train_image ,
                      k = 10, loss = c("accuracy", "roc"))

# BOOSTED TREES 
image_boost <- CVmaster2(model = "rf_boost", X = X_train_blocks, y = y_train_blocks ,
                      k = 10, loss = c("accuracy", "roc")) 

# SVM 
# LDA 
# QDA 



## RUN BLOCK MODELS 
# LOGISTIC 
block_log <- CVmaster2(model = "logistic", X = X_train_blocks, y =y_train_blocks, 
                       k = 10, loss = c("accuracy", "roc"))
# RF
block_rf <- CVmaster2(model = "rf", X = X_train_blocks, y = y_train_blocks ,
                      k = 10, loss = c("accuracy", "roc"))

# BOOSTED TREES 
block_boost <- CVmaster2(model = "rf_boost", X = X_train_blocks, y = y_train_blocks ,
                      k = 10, loss = c("accuracy", "roc")) 

# SVM 
# LDA 
# QDA 

```

```{r}
## GET ACCURACY ESTIMATES BY MODEL 
image_log$accuracy %>%
  bind_rows(image_rf$accuracy) %>%
  bind_rows(image_boost$accuracy) %>%
  mutate(type = "Image") %>%
  bind_rows(block_log$accuracy) %>%
  bind_rows(block_rf$accuracy) %>%
  bind_rows(block_boost$accuracy) %>%
  mutate(type = ifelse(is.na(type), "Block", type)) %>%
  group_by(type,model) %>%
  rename(accuracy = mean) %>%
  mutate(mean_accuracy = mean(accuracy)) %>%
  pivot_wider(names_from = id, values_from = accuracy) 

# add svm, lda, qda
# reformat the tuning parameter columns, take out metric, reorder, general formatting

  
```


#### Part B 
```{r}
## GET ROC CURVES BY MODEL 

image_log$roc %>%
  bind_rows(image_rf$roc) %>%
  bind_rows(image_boost$roc) +
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() 


  block_log$roc %>%
  bind_rows(block_rf$roc) %>%
  bind_rows(block_boost$roc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path(lwd = 0.5, alpha = 0.6) +
  geom_abline(lty = 3) + 
  coord_equal() 

# add svm, qda, lda
# change colors, linetype, size

```

#### Part C 
```{r}
## PRECISION ESTIMATES 

## PLOTTING PREDICTIONS BY COORDS 
```



```{r}
# LDA 
image_lda_accuracy <- CVmaster("lda", X_train_image , y_train_image ,10,"accuracy")
# QDA
image_qda_accuracy <- CVmaster("qda", X_train_image , y_train_image ,10,"accuracy")
# LOGISTIC 
image_log_accuracy <- CVmaster("logistic", X_train_image , y_train_image ,10,"accuracy")

# RF
image_rf_accuracy <- CVmaster("rf", X_train_image , y_train_image ,10,"accuracy",ntree=5)


# fix table 
# data.frame(type = "lda", acc = image_lda_accuracy, folds = 1:10) %>%
#   bind_rows(data.frame(type = "qda", acc = image_qda_accuracy, folds = 1:10)) %>%
#   bind_rows(data.frame(type = "log", acc = image_log_accuracy, folds = 1:10)) %>%
#   group_by(type) %>%
#   mutate(mean_acc = mean(acc)) %>%
#   pivot_wider(names_from = folds, values_from = acc) -> image_accuracy_table
```

```{r}

image_log_test <- CVmaster2(model = "logistic", X = X_train_image, y =y_train_image, 
                       k = 10, loss = c("accuracy", "roc"))


image_log_test$.metrics
  
block_boost$roc


```

Second way of splitting the data by blocks
```{r}
# LDA 
block_lda_accuracy <- CVmaster("lda", X_train_blocks , y_train_blocks ,10,"accuracy")
# QDA
block_qda_accuracy <- CVmaster("qda", X_train_blocks , y_train_blocks ,10,"accuracy")
# LOGISTIC 
block_log_accuracy <- CVmaster("logistic", X_train_blocks , y_train_blocks ,10,"accuracy")
# RF
block_rf_accuracy <- CVmaster("rf", X_train_blocks , y_train_blocks ,10,"accuracy")

# data.frame(type = "lda", acc = block_lda_accuracy, folds = 1:10) %>%
#   bind_rows(data.frame(type = "qda", acc = block_qda_accuracy, folds = 1:10)) %>%
#   bind_rows(data.frame(type = "log", acc = block_log_accuracy, folds = 1:10)) %>%
#   group_by(type) %>%
#   mutate(mean_acc = mean(acc)) %>%
#   pivot_wider(names_from = folds, values_from = acc) -> block_accuracy_table

library(plotROC)
lda_tester <- CVmaster("lda", X_train_blocks , y_train_blocks, 10, "accuracy")

lda_tester %>%
  mutate(ylabels = as.integer(ylabels)) %>%
  ggplot(aes(m = probs.1, d = ylabels)) + geom_roc( labels = FALSE)
```


















