---
title: "521 Project 2"
output: html_document
---

```{r setup, include=FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)

library(tidyverse)
library(viridis)
library(GGally)
library(tidymodels)
library(patchwork)
# library(gridExtra)
library(kableExtra)
library(ggpubr)
source("CVmaster.R")
options(scipen = 999)


# graph theme object 
theme_settings <- theme_light() + theme(axis.text = element_text(size=16), axis.title = element_text(size = 17), legend.title = element_text(size = 17), plot.title = element_text(size = 16, hjust = 0.5), legend.text = element_text(size = 17)) 
```

## Section 1 

&nbsp; 

```{r}
# read in files with column names 
names <- c("y_coord", "x_coord", "expert", "ndai", "sd", "corr", "angle_df", 
           "angle_cf", "angle_bf", "angle_af", "angle_an")

imagem1 <- read.table('./data/imagem1.txt', col.names = names)
imagem2 <- read.table('./data/imagem2.txt', col.names = names)
imagem3 <- read.table('./data/imagem3.txt', col.names = names)


# make expert column factor and text version
imagem1 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem1

imagem2 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem2

imagem3 %>%
  mutate(expert = as.factor(expert)) %>%
  mutate(expert_text = ifelse(expert == 0, "unlabeled", ifelse(expert == 1, "cloudy","clear"))) -> imagem3

imagem1 %>%
  bind_rows(imagem2) %>%
  bind_rows(imagem3) -> all

```

&nbsp; 

#### Part B

&nbsp; 

```{r}
# # look at percentage for total 
# all %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / 345556),2))
# 
# # look at percentage for each one 
# len1 <- nrow(imagem1)
# imagem1 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len1),2))
# 
# len2 <- nrow(imagem2)
# imagem2 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len2),2))
# 
# len3 <- nrow(imagem3)
# imagem3 %>%
#   group_by(expert_text) %>%
#   summarise(prop_total = round((n() / len3),2))
```

```{r}
# # make summary table 
# overall <- c("37%", "23%", "40%")
# image1 <- c("37%", "34%", "29%")
# image2 <- c("44%", "18%", "38%")
# image3 <- c("29%", "18%", "52%")
# type <- c("Clear", "Cloudy", "Unlabeled")
# bind_cols(type, overall) %>%
#   bind_cols(image1) %>%
#   bind_cols(image2) %>%
#   bind_cols(image3)  -> summary_table
# 
# names(summary_table) <- c("Type", "Overall", "Image 1", "Image 2", "Image 3")
# 
# 
# summary_table %>%
#   kbl() %>%
#   kable_classic(full_width = F, html_font = "Cambria")
```


```{r fig.width = 9, fig.height=3}

# We might want to change the colors here becuase it's hard to distinguish between the blue and the black on the legend

# Image 1
p1 <- ggplot(imagem1, aes(x = x_coord, y = y_coord, color = expert_text)) +
  geom_point() +
  theme_settings +
  labs(x = "", y = "Y Coordinate", color = "Expert Labels") +
  scale_colour_manual(labels = c("Clear", "Cloudy", "Unlabeled"), values = c("#132B43", "#56B1F7", "purple")) +
  guides(color = guide_legend(override.aes = list(size = 4))) + 
  ggtitle('Image 1')
  

# Image 2
p2 <- ggplot(imagem2, aes(x = x_coord, y = y_coord, color = expert_text)) +
  geom_point() +
  theme_settings +
  labs(x = "X Coordinate", y = "", color = "Expert Labels") +
  scale_colour_manual(labels = c("Clear", "Cloudy", "Unlabeled"), values = c("#132B43", "#56B1F7", "purple")) +
  guides(color = guide_legend(override.aes = list(size = 4))) + 
  ggtitle('Image 2')

# Image 3
p3 <- ggplot(imagem3, aes(x = x_coord, y = y_coord, color = expert_text)) +
  geom_point() +
  theme_settings +
  labs(x = "", y = "", color = "Expert Labels") +
  scale_colour_manual(labels = c("Clear", "Cloudy", "Unlabeled"), values = c("#132B43", "#56B1F7", "purple")) +
  guides(color = guide_legend(override.aes = list(size = 4))) + 
  ggtitle('Image 3')

p1 + p2 + p3 + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```

&nbsp; 

#### Part C

&nbsp; 

```{r}
# pairwise relationships between features 
# cor(all$ndai, all$corr)

# all %>%
#   select(ndai:angle_an) %>%
#   ggpairs()
```


```{r}
# # expert labels vs features 
# p1 <- imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = ndai, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) + 
#   labs(x = "NDAI", y = "Count", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p2 <- imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = corr, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) + 
#   labs(x = "CORR", y = "", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p3 <- imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = sd, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) +
#   labs(x = "SD", y = "", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p4 <- imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_df, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) +
#   labs(x = "Angle DF", y = "Count", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p5 <- imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_cf, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) +
#   labs(x = "Angle CF", y = "", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p6 <- imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_bf, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) +
#   labs(x = "Angle BF", y = "", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p7 <-imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_af, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) +
#   labs(x = "Angle AF", y = "Count", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p8 <- imagem1 %>%
#   filter(expert != 0) %>%
#   ggplot(aes(x = angle_an, fill= expert)) +
#   geom_histogram() + 
#   scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7")) +
#   labs(x = "Angle AN", y = "", fill = "Labels") + 
#   theme_light() +
#   theme(axis.text = element_text(size=7), axis.title = element_text(size = 8), legend.title = element_text(size = 9), legend.text=element_text(size=8)) 
# 
# p1 + p2 + p3 + p4 + p5+ p6+ p7 + p8 + plot_layout(guides = "collect") 

```


## Section 2

#### Part A

```{r}
set.seed(513)
images <- list(imagem1,imagem2,imagem3)
indices <- sample(1:3)
train_image <- images[[indices[1]]]
val_image <- images[[indices[2]]]
test_image <- images[[indices[3]]]
```

```{r}
find_blocks <- function(x, y, num_blocks) {

  n <- sqrt(num_blocks)
  x_min <- min(x)
  x_max <- max(x)
  x_step <- (x_max-x_min)/n
  
  y_min <- min(y)
  y_max <- max(y)
  y_step <- (y_max-y_min)/n
  
  block <- tibble()

  
  for (i in seq_len(n)) {
    # make top and bottom
    top <- floor(y_min + (i-1) * y_step)
    bot <- floor(y_max - (n-i) * y_step )
    if (i != n)
      bot <- bot - 1
    
    for (j in seq_len(n)){
      # make left and right 
      left <- floor(x_min + (j-1) * x_step)
      right <- floor(x_max - (n-j) * x_step )
      if (j != n)
        right <- right - 1
      
      block %>%
        bind_rows(data.frame(top, bot, left, right)) -> block 
    }
  }
  names(block) <- c("top", "bottom", "left", "right")
  return(block)
}

# this is going to return overlapping intervals, so will need x >= top and x < bot, etc. 
```



```{r}
split_blocks <- function(block, num_blocks, train_num_blocks, val_num_blocks) {
  set.seed(513)
  train_index <- sample(seq_len(num_blocks),train_num_blocks)
  set.seed(513)
  val_index <- sample(setdiff(seq_len(num_blocks),train_index),val_num_blocks)
  test_index <- setdiff(seq_len(num_blocks), c(train_index,val_index))
  
  train_blocks <- block[train_index,]
  val_blocks <- block[val_index,]
  test_blocks <- block[test_index,]
  
  return(list(train = train_blocks, val = val_blocks, test = test_blocks))
}
```

```{r}
split_data <- function(data, block) {
  final_data <- tibble()
  for (i in seq_len(nrow(block))) {
    coords <- block[i,]
    
    data %>%
      filter(x_coord >= coords$left, x_coord <= coords$right,
            y_coord >= coords$top, y_coord <= coords$bottom) -> filtered_data
        
    final_data <- rbind(final_data, filtered_data)
  }
  
  return(final_data)
}
```


```{r}
process_data_main <- function(df, num_blocks, train_num_blocks, val_num_blocks) {
  blocks <- find_blocks(df$x_coord, df$y_coord, num_blocks)
  block_indices <- split_blocks(blocks, num_blocks, train_num_blocks, val_num_blocks)
  
  val <- split_data(df, block_indices$val)
  test <- split_data(df, block_indices$test)
  train <- split_data(df, block_indices$train)

  return(list(val = val, test = test, train = train))
}
```


```{r}
image1_dfs <- process_data_main(imagem1, 16, 10, 3)
image2_dfs <- process_data_main(imagem2, 16, 10, 3)
image3_dfs <- process_data_main(imagem3, 16, 10, 3)

train <- image1_dfs$train %>%
  bind_rows(image2_dfs$train) %>%
  bind_rows(image3_dfs$train)

test <- image1_dfs$test%>%
  bind_rows(image2_dfs$test) %>%
  bind_rows(image3_dfs$test)


val <- image1_dfs$val%>%
  bind_rows(image2_dfs$val) %>%
  bind_rows(image3_dfs$val)

# 345556
# nrow(train) + nrow(test) + nrow(val)
```

#### Part B
```{r}
val_image %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
test_image %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))

val %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
test %>%
  filter(expert != 0)%>%
  summarise(mean(expert == -1))
```
  
This classifier will have high average accuracy when the image is mostly cloud free.

#### Part C
```{r}
all %>%
  dplyr::select(ndai:angle_an) %>%
  cor(.,as.integer(all$expert)) -> correlation_table

names <- c("NDAI", "SD", "CORR", "Angle DF", "Angle CF", "Angle BF", "Angle AF", "Angle AN")

tibble(names, correlation_table) %>%
  mutate(correlation_table = round(correlation_table, 4)) %>%
  rename(Predictor = names, `Correlation` = correlation_table)  %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r}
imagem1 %>%
  filter(expert != 0) %>%
  ggplot(aes(x = sd, fill= expert)) +
  geom_histogram()
```
    
NDAI and CORR are selected as best features becuase they have the highest correlation values with the expert labels. SD is also selected because the range of values between the cloudy and cloud free data is more separable than the range of values for the angles.

#### Part D
See the CVmaster.R script.

## Section 3

#### Part A

```{r}
# get x and y train data for each data split
X_train_image <- train_image %>%
  bind_rows(val_image) %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text, -expert)

y_train_image  <- train_image %>%
  bind_rows(val_image) %>%
  filter(expert!=0) %>%
  .$expert %>%
  droplevels()

X_train_blocks <- train %>%
  bind_rows(val) %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text, -expert)

y_train_blocks  <- train %>%
  bind_rows(val) %>%
  filter(expert!=0) %>%
  .$expert %>%
  droplevels()
```


First modelling by splitting the data by image and by block 
```{r}
## RUN IMAGE MODELS 

# LOGISTIC 
image_log <- CVmaster(model = "logistic", 
                       X = X_train_image, y =y_train_image, 
                       k = 10, loss = c("accuracy"), 
                       estimates = c("roc", "conf_mat", "precision"))

# RF
image_rf <- CVmaster(model = "rf",
                      X = X_train_image, y = y_train_image,
                      k = 10, loss = c("accuracy"), 
                      estimates = c("roc", "conf_mat", "precision"))

# BOOSTED TREES 
image_boost <- CVmaster(model = "boosted_trees", 
                         X = X_train_image, y = y_train_image,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 


# LDA 
image_lda <- CVmaster(model = "lda", 
                         X = X_train_image, y = y_train_image,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
# QDA 
image_qda <- CVmaster(model = "qda", 
                         X = X_train_image, y = y_train_image,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision"))


## RUN BLOCK MODELS 
# LOGISTIC 
block_log <- CVmaster(model = "logistic", 
                       X = X_train_blocks, y =y_train_blocks, 
                       k = 10, loss = c("accuracy"), 
                       estimates = c("roc", "conf_mat", "precision"))
# RF
block_rf <- CVmaster(model = "rf", 
                      X = X_train_blocks, y = y_train_blocks,
                      k = 10, loss = c("accuracy"), 
                      estimates = c("roc", "conf_mat", "precision"))

# BOOSTED TREES 
block_boost <- CVmaster(model = "boosted_trees", 
                         X = X_train_blocks, y = y_train_blocks,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
# LDA 
block_lda <- CVmaster(model = "lda", 
                         X = X_train_blocks, y = y_train_blocks,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
# QDA 
block_qda <- CVmaster(model = "qda", 
                         X = X_train_blocks, y = y_train_blocks,
                         k = 10, loss = c("accuracy"), 
                         estimates = c("roc", "conf_mat", "precision")) 
```


```{r}
## GET ACCURACY ESTIMATES BY MODEL 
image_log$accuracy %>%
  bind_rows(image_rf$accuracy) %>%
  bind_rows(image_boost$accuracy) %>%
  bind_rows(image_lda$accuracy) %>%
  bind_rows(image_qda$accuracy) %>%
  mutate(type = "Image Split") %>%
  bind_rows(block_log$accuracy) %>%
  bind_rows(block_rf$accuracy) %>%
  bind_rows(block_boost$accuracy) %>%
  bind_rows(block_lda$accuracy) %>%
  bind_rows(block_qda$accuracy) %>%
  mutate(type = ifelse(is.na(type), "Block Split", type)) -> model_sum

model_sum %>%
  dplyr::select(-mtry, -min_n, -penalty, -metric) %>%
  group_by(type,model) %>%
  rename(accuracy = mean) %>%
  mutate(mean_accuracy = round(mean(accuracy), 4)) %>%
  mutate(accuracy = round(accuracy, 4)) %>%
  mutate(id = str_replace(id, "Fold", "Fold ")) %>%
  pivot_wider(names_from = id, values_from = accuracy) %>%
  arrange(desc(mean_accuracy)) %>%
  rename(`Mean Accuracy` = mean_accuracy)%>%
  rename_with(str_to_title) -> model_accuracy_table

model_accuracy_table %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r}
# BEST TUNING PARAMETER 
model_sum %>%
  ungroup() %>%
  dplyr::select(-metric, -mean, -id) %>%
  distinct() %>%
  # combine Min Try / Nodes 
  mutate(`Min Try / Min Nodes` = str_c(mtry, "/", min_n)) %>%
  mutate(penalty = as.character(round(penalty, 5))) %>%
  # rename(`Min Try` = mtry, 
  #        `Min Nodes` = min_n) %>%
  rename_with(str_to_title) %>%
  dplyr::select(-Mtry, -Min_n) %>%
  # rename(`Min Nodes` = min_n) %>%
  pivot_longer(cols = c(Penalty, `Min Try / Min Nodes`), names_to = "Parameter", values_to = "Value")  %>%
  #pivot wider by type 
  filter(!is.na(Value)) %>%
  pivot_wider(names_from = Type, values_from = Value) -> parameters_table


parameters_table %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

#### Part B 
```{r fig.width=5, fig.height=3}
## GET ROC CURVES BY MODEL 
image_roc_plot <- image_log$roc %>%
  bind_rows(image_rf$roc) %>%
  bind_rows(image_boost$roc) %>%
  bind_rows(image_lda$roc) %>%
  bind_rows(image_qda$roc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path() +
  geom_abline(lty = 3) + 
  geom_vline(xintercept = 0.5, color = "red", size = 0.5) + 
  coord_equal() +
  theme_settings + 
  labs(x = "1 - Specificity", y = "Sensitivity", col = "Model") + 
  guides(color = guide_legend(override.aes = list(size = 4))) +
  ggtitle('Image Split')


block_roc_plot<-  block_log$roc %>%
  bind_rows(block_rf$roc) %>%
  bind_rows(block_boost$roc) %>%
    bind_rows(block_lda$roc) %>%
  bind_rows(block_qda$roc) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + 
  geom_path() +
  geom_abline(lty = 3) + 
  geom_vline(xintercept = 0.5, color = "red", size = 0.5) + 
  coord_equal() + 
  theme_settings +
  labs(x = "1 - Specificity", y = "", col = "Model")+ 
  guides(color = guide_legend(override.aes = list(size = 4))) +
  ggtitle('Block Split')

# change colors, linetype, size
image_roc_plot + block_roc_plot + plot_layout(guides = "collect") & theme(legend.position = 'bottom')

```

#### Part C 
```{r}
## PRECISION ESTIMATES 
image_precision <- round(c(image_log$precision, image_rf$precision,image_boost$precision,image_lda$precision,image_qda$precision),4)
block_precision <- round(c(block_log$precision, block_rf$precision,block_boost$precision,block_lda$precision,block_qda$precision),4)
models <- c("Logistic Regression", "Random Forest", "Boosted Trees", "LDA", "QDA")

bind_cols(Model = models, `Image Split` = image_precision) %>%
  bind_cols(`Block Split` = block_precision) %>%
  arrange(desc(`Block Split`)) -> precision_table

precision_table %>%
  kbl() %>%
  kable_classic(full_width = F, html_font = "Cambria")
```

```{r fig.width = 12}
## CONF MATRIX IMAGE
p1 <- tibble(image_log$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("Logistic Regression")

p2 <- tibble(image_rf$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("Random Forest")

p3 <- tibble(image_boost$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("Boosted Trees")

p4 <- tibble(image_lda$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("LDA")

p5 <- tibble(image_qda$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("QDA")
  
  
p1 + p2 + p3 + p4 + p5 + plot_layout(guides = "collect") & theme(legend.position = 'bottom')

```

```{r fig.width = 12}
## CONFUSION MATRIX BLOCK
p1 <- tibble(block_log$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("Logistic Regression")

p2 <- tibble(block_rf$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("Random Forest")

p3 <- tibble(block_boost$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("Boosted Trees")

p4 <- tibble(block_lda$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("LDA")

p5 <- tibble(block_qda$conf_mat) %>% 
  mutate(`Predicted Label` = c("Clear", "Cloudy")) %>%
  rename(Clear = `truth_-1`, Cloudy = truth_1) %>%
  pivot_longer(cols = `Clear`:Cloudy, names_to = "True Label")%>%
  ggplot(aes(x = `Predicted Label`, y = `True Label`)) + 
  geom_tile(aes(fill = value))+
  geom_text(aes(label = sprintf("%.2f", value)), vjust = 1, color = "white", size = 9) +
  scale_fill_gradient(limits = c(0, 0.65)) +
  theme_settings + 
  ggtitle("QDA")
  
  
p1 + p2 + p3 + p4 + p5 + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```


## Section 4

```{r}
library(ranger)
```

```{r}
rf_train_block <- train %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text)

rf_val_block <- val %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text)

rf_test_block <- test %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text)
```

```{r}
mtrys <- 1:8
min_ns <- 1:10

block_acc <- matrix(NA,nrow=length(mtrys),ncol=length(min_ns))
for (i in seq_along(mtrys)){
  for (j in seq_along(min_ns)){
    mod <- ranger(expert ~., data = rf_train_block,mtry = mtrys[i], min_n = min_ns[j], trees = 50)
    preds <- predict(mod,data=rf_val_block)
    block_acc[i,j] <- mean(rf_val_block$expert==preds$predictions)
  }
}
```

```{r}
library(RColorBrewer)
heatmap(block_acc, Colv = NA, Rowv = NA,col=colorRampPalette(brewer.pal(8, "Blues"))(25))

block_accuracy <- as.data.frame(block_acc)
block_accuracy$mtry <- 1:8
block_accuracy <- block_accuracy %>%
  pivot_longer(!mtry,names_to = "min_node", values_to = "Accuracy") %>%
  mutate(min_node = as.integer(sub(".", "", min_node)))

ggplot(block_accuracy, aes(mtry,min_node,fill=Accuracy)) + 
  geom_tile() +
  scale_y_continuous(breaks=c(2,4,6,8,10))+theme_light()+
  theme(axis.text = element_text(size=9), axis.title = element_text(size = 9), 
        legend.title = element_text(size = 9), plot.title = element_text(size = 12, hjust = 0.5))+
  labs(x="Number of Variables Available",y="Minimum Node Size", title = "Validation Accuracy")
```
Based on this, picking 4 for both mtry and min_n (0.9138478)

```{r}
rf_mod <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 100)
preds <- predict(mod,data=rf_val_block)
mean(rf_val_block$expert==preds$predictions)
```

```{r}
rf_mod <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 250)
preds <- predict(mod,data=rf_val_block)
mean(rf_val_block$expert==preds$predictions)
```

```{r}
rf_mod <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 500)
preds <- predict(mod,data=rf_val_block)
mean(rf_val_block$expert==preds$predictions)
```

Increasing number of trees does not significantly change predictions

```{r}
set.seed(513)
final_mod <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 50,importance = 'impurity')
val_preds <- predict(final_mod,data=rf_val_block)
mean(rf_val_block$expert==val_preds$predictions)

test_preds <- predict(final_mod,data=rf_test_block)
mean(rf_test_block$expert==test_preds$predictions)
```

```{r}
final_mod_prob <- ranger(expert ~., data = rf_train_block,mtry = 4, min_n = 4, trees = 50,probability = TRUE)
test_prob_preds <- predict(final_mod_prob,data=rf_test_block)
```

```{r}
var_imp <- tibble(variable=names(final_mod$variable.importance),importance = final_mod$variable.importance)

 ggplot(var_imp, aes(x=reorder(variable,importance), y=importance,fill=importance))+ 
      geom_bar(stat="identity", position="dodge")+ coord_flip()+ theme_light()+
      ylab("Variable Importance")+
      xlab("")+
      guides(fill=F)
```

```{r}
image_1_data <- imagem1 %>%
  select(-y_coord, -x_coord, -expert_text)
image_1_class <- predict(final_mod,data=image_1_data)$predictions
image_1_pred_class <- tibble(x_coord = imagem1$x_coord, y_coord = imagem1$y_coord, class = image_1_class)
ggplot(image_1_pred_class, aes(x_coord,y_coord, fill = class)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))

image_1_preds <- predict(final_mod_prob,data=image_1_data)$predictions
image_1_probs <- tibble(x_coord = imagem1$x_coord, y_coord = imagem1$y_coord, Probability = image_1_preds[,2])
ggplot(image_1_probs, aes(x_coord,y_coord, fill = Probability)) + 
     geom_raster()+labs(x="X Coordinate", y="Y Coordinate", title = "Image 1 Cloudiness Probability")+
  theme_light()+
  theme(axis.text = element_text(size=10), axis.title = element_text(size = 10), 
        legend.title = element_text(size = 10), plot.title = element_text(size = 12, hjust = 0.5))
```

```{r}
image_2_data <- imagem2 %>%
  select(-y_coord, -x_coord, -expert_text)
image_2_class <- predict(final_mod,data=image_2_data)$predictions
image_2_pred_class <- tibble(x_coord = imagem2$x_coord, y_coord = imagem2$y_coord, class = image_2_class)
ggplot(image_2_pred_class, aes(x_coord,y_coord, fill = class)) + 
     geom_raster()+scale_fill_manual(values = c("#132B43", "#56B1F7"))

image_2_preds <- predict(final_mod_prob,data=image_2_data)$predictions
image_2_probs <- tibble(x_coord = imagem2$x_coord, y_coord = imagem2$y_coord, Probability = image_2_preds[,2])
ggplot(image_2_probs, aes(x_coord,y_coord, fill = Probability)) + 
     geom_raster()
```

```{r}
image_3_data <- imagem3 %>%
  select(-y_coord, -x_coord, -expert_text)
image_3_class <- predict(final_mod,data=image_3_data)$predictions
image_3_pred_class <- tibble(x_coord = imagem3$x_coord, y_coord = imagem3$y_coord, class = image_3_class)
ggplot(image_3_pred_class, aes(x_coord,y_coord, fill = class)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))

image_3_preds <- predict(final_mod_prob,data=image_3_data)$predictions
image_3_probs <- tibble(x_coord = imagem3$x_coord, y_coord = imagem3$y_coord, Probability = image_3_preds[,2])
ggplot(image_3_probs, aes(x_coord,y_coord, fill = Probability)) + 
     geom_raster()
```

```{r,fig.width=7,fig.height=3.2}
image_1_plot <- ggplot(image_1_probs, aes(x_coord,y_coord, fill = Probability)) + 
     geom_raster()+labs(x="X Coordinate", y="Y Coordinate", title = "Image 1")+
  theme_light()+
  theme(axis.text = element_text(size=10), axis.title = element_text(size = 10), 
        legend.title = element_text(size = 12), plot.title = element_text(size = 12, hjust = 0.5))+
  theme(axis.title.x = element_blank())
image_2_plot <- ggplot(image_2_probs, aes(x_coord,y_coord, fill = Probability)) + 
     geom_raster()+labs(x="X Coordinate", y="Y Coordinate", title = "Image 2")+
  theme_light()+
  theme(axis.text = element_text(size=10), axis.title = element_text(size = 10), 
        legend.title = element_text(size = 12), plot.title = element_text(size = 12, hjust = 0.5))+
  theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(),
                                        axis.title.y = element_blank())
image_3_plot <- ggplot(image_3_probs, aes(x_coord,y_coord, fill = Probability)) + 
     geom_raster()+labs(x="X Coordinate", y="Y Coordinate", title = "Image 3")+
  theme_light()+ theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(),
                       axis.title.y = element_blank(),axis.title.x = element_blank())+
  theme(axis.text = element_text(size=10), axis.title = element_text(size = 10), 
        legend.title = element_text(size = 12), plot.title = element_text(size = 12, hjust = 0.5))
image_1_plot + image_2_plot + image_3_plot + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```


```{r}
image_1_pred_class <- image_1_pred_class %>%
  mutate(misclassified = ifelse(imagem1$expert != 0 & class != imagem1$expert,"Yes","No"))

ggplot(image_1_pred_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r}
image_2_pred_class <- image_2_pred_class %>%
  mutate(misclassified = ifelse(imagem2$expert != 0 & class != imagem2$expert,"Yes","No"))

ggplot(image_2_pred_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r}
image_3_pred_class <- image_3_pred_class %>%
  mutate(misclassified = ifelse(imagem3$expert != 0 & class != imagem3$expert,"Yes","No"))

ggplot(image_3_pred_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r,fig.width=7,fig.height=3}
misclass_1 <- ggplot(image_1_pred_class, aes(x_coord,y_coord, fill = class,alpha=misclassification)) + 
  geom_raster()+ scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7"))+ scale_alpha_identity()+
  geom_hline(yintercept = 2,alpha=0.5)+geom_hline(yintercept = 97,alpha=0.5)+geom_hline(yintercept = 192,alpha=0.5)+
  geom_hline(yintercept = 287,alpha=0.5)+geom_hline(yintercept = 383,alpha=0.5)+
  geom_vline(xintercept=65,alpha=0.5)+geom_vline(xintercept=140,alpha=0.5)+geom_vline(xintercept=216,alpha=0.5)+
  geom_vline(xintercept=292,alpha=0.5)+geom_vline(xintercept=368,alpha=0.5)+
  labs(x="X Coordinate", y="Y Coordinate", title = "Image 1",fill="Prediction")+
  theme_light()+
  theme(axis.text = element_text(size=10), axis.title = element_text(size = 10), 
        legend.title = element_text(size = 12), plot.title = element_text(size = 12, hjust = 0.5))+
  theme(axis.title.x = element_blank())

misclass_2 <- ggplot(image_2_pred_class, aes(x_coord,y_coord, fill = class,alpha=misclassification)) + 
  geom_raster()+ scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7"))+ scale_alpha_identity()+
  geom_hline(yintercept = 2,alpha=0.5)+geom_hline(yintercept = 97,alpha=0.5)+geom_hline(yintercept = 192,alpha=0.5)+
  geom_hline(yintercept = 287,alpha=0.5)+geom_hline(yintercept = 383,alpha=0.5)+
  geom_vline(xintercept=65,alpha=0.5)+geom_vline(xintercept=141,alpha=0.5)+geom_vline(xintercept=217,alpha=0.5)+
  geom_vline(xintercept=293,alpha=0.5)+geom_vline(xintercept=369,alpha=0.5)+
  labs(x="X Coordinate", y="Y Coordinate", title = "Image 2",fill="Prediction")+
  theme_light()+
  theme(axis.text = element_text(size=10), axis.title = element_text(size = 10), 
        legend.title = element_text(size = 12), plot.title = element_text(size = 12, hjust = 0.5))+
  theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(),
                                        axis.title.y = element_blank())

misclass_3 <- ggplot(image_3_pred_class, aes(x_coord,y_coord, fill = class,alpha=misclassification)) + 
  geom_raster() + scale_fill_manual(labels = c("Clear", "Cloudy"), values = c("#132B43", "#56B1F7"))+ scale_alpha_identity()+
  geom_hline(yintercept = 2,alpha=0.5)+geom_hline(yintercept = 97,alpha=0.5)+geom_hline(yintercept = 192,alpha=0.5)+
  geom_hline(yintercept = 287,alpha=0.5)+geom_hline(yintercept = 383,alpha=0.5)+
  geom_vline(xintercept=65,alpha=0.5)+geom_vline(xintercept=141,alpha=0.5)+geom_vline(xintercept=217,alpha=0.5)+
  geom_vline(xintercept=293,alpha=0.5)+geom_vline(xintercept=369,alpha=0.5)+
  labs(x="X Coordinate", y="Y Coordinate", title = "Image 3",fill="Prediction")+
  theme_light()+
  theme(axis.text = element_text(size=10), axis.title = element_text(size = 10), 
        legend.title = element_text(size = 12), plot.title = element_text(size = 12, hjust = 0.5))+
  theme(axis.text.y = element_blank(),axis.ticks.y = element_blank(),
                       axis.title.y = element_blank(),axis.title.x = element_blank())
misclass_1 + misclass_2 + misclass_3 + plot_layout(guides = "collect") & theme(legend.position = 'bottom')
```

```{r}
# Again but with image split

rf_train_image <- train_image %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text)

rf_val_image <- val_image %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text)

rf_test_image <- test_image %>%
  filter(expert != 0) %>%
  dplyr::select(-y_coord, -x_coord, -expert_text)
```

```{r}
mtrys <- 1:8
min_ns <- 1:10

image_acc <- matrix(NA,nrow=length(mtrys),ncol=length(min_ns))
for (i in seq_along(mtrys)){
  for (j in seq_along(min_ns)){
    mod <- ranger(expert ~., data = rf_train_image,mtry = mtrys[i], min_n = min_ns[j], trees = 50)
    preds <- predict(mod,data=rf_val_image)
    image_acc[i,j] <- mean(rf_val_image$expert==preds$predictions)
  }
}
```

```{r}
image_accuracy <- as.data.frame(image_acc)
image_accuracy$mtry <- 1:8
image_accuracy <- image_accuracy %>%
  pivot_longer(!mtry,names_to = "min_node", values_to = "Accuracy") %>%
  mutate(min_node = as.integer(sub(".", "", min_node)))

ggplot(image_accuracy, aes(mtry,min_node,fill=Accuracy)) + 
  geom_tile() +
  scale_y_continuous(breaks=c(2,4,6,8,10))+theme_light()+
  theme(axis.text = element_text(size=9), axis.title = element_text(size = 9), 
        legend.title = element_text(size = 9), plot.title = element_text(size = 12, hjust = 0.5))+
  labs(x="Number of Variables Available",y="Minimum Node Size", title = "Validation Accuracy")
```
  
Based on this, pick 2 for mtry and 6 for min node (note accuracy is significantly worse), 0.6767066	

```{r}
set.seed(513)
final_image_mod <- ranger(expert ~., data = rf_train_image,mtry = 2, min_n = 6, trees = 50,importance = 'impurity')
val_image_preds <- predict(final_image_mod,data=rf_val_image)
mean(rf_val_image$expert==val_image_preds$predictions)

test_image_preds <- predict(final_image_mod,data=rf_test_image)
mean(rf_test_image$expert==test_image_preds$predictions)
```

Accuracy significantly worse

```{r}
var_imp_image <- tibble(variable=names(final_image_mod$variable.importance),importance = final_image_mod$variable.importance)

 ggplot(var_imp_image, aes(x=reorder(variable,importance), y=importance,fill=importance))+ 
      geom_bar(stat="identity", position="dodge")+ coord_flip()+
      ylab("Variable Importance")+
      xlab("")+
      guides(fill=F)
```

Same variable importance

```{r}
image_1_image_preds <- predict(final_image_mod,data=image_1_data)$predictions
image_1_image_class <- tibble(x_coord = imagem1$x_coord, y_coord = imagem1$y_coord, class = image_1_image_preds)
image_1_image_class <- image_1_image_class %>%
  mutate(misclassified = ifelse(imagem1$expert != 0 & class != imagem1$expert,"Yes","No"))

ggplot(image_1_image_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```
```{r}
image_2_image_preds <- predict(final_image_mod,data=image_2_data)$predictions
image_2_image_class <- tibble(x_coord = imagem2$x_coord, y_coord = imagem2$y_coord, class = image_2_image_preds)
image_2_image_class <- image_2_image_class %>%
  mutate(misclassified = ifelse(imagem2$expert != 0 & class != imagem2$expert,"Yes","No"))

ggplot(image_2_image_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r}
image_3_image_preds <- predict(final_image_mod,data=image_3_data)$predictions
image_3_image_class <- tibble(x_coord = imagem3$x_coord, y_coord = imagem3$y_coord, class = image_3_image_preds)
image_3_image_class <- image_3_image_class %>%
  mutate(misclassified = ifelse(imagem3$expert != 0 & class != imagem3$expert,"Yes","No"))

ggplot(image_3_image_class, aes(x_coord,y_coord, fill = misclassified)) + 
     geom_raster()+ scale_fill_manual(values = c("#132B43", "#56B1F7"))
```

```{r}
final_image_mod$prediction.error
```


```{r}
library(rpart)
library(rpart.plot)
rf_train_block$expert <- droplevels(rf_train_block$expert)
mod_tree <- rpart(expert ~., data = rf_train_block)
rpart.plot(mod_tree)
```


